import argparse
from time import time
import sys

import torch.optim as optim
from torch.autograd import Variable

from caser import Caser
from evaluation import evaluate_ranking
from interactions import Interactions
from utils import *


class Recommender(object):
    """
    Contains attributes and methods that needed to train a sequential
    recommendation model. Models are trained by many tuples of
    (users, sequences, targets, negatives) and negatives are from negative
    sampling: for any known tuple of (user, sequence, targets), one or more
    items are randomly sampled to act as negatives.


    Parameters
    ----------

    n_iter: int,
        Number of iterations to run.
    batch_size: int,
        Minibatch size.
    l2: float,
        L2 loss penalty, also known as the 'lambda' of l2 regularization.
    neg_samples: int,
        Number of negative samples to generate for each targets.
        If targets=3 and neg_samples=3, then it will sample 9 negatives.
    learning_rate: float,
        Initial learning rate.
    use_cuda: boolean,
        Run the model on a GPU or CPU.
    model_args: args,
        Model-related arguments, like latent dimensions.
    """

    def __init__(self,
                 n_iter=None,
                 batch_size=None,
                 l2=None,
                 neg_samples=None,
                 learning_rate=None,
                 use_cuda=False,
                 model_args=None):

        # model related
        self._num_items = None
        self._num_users = None
        self._net = None
        self.model_args = model_args

        # learning related
        self._batch_size = batch_size
        self._n_iter = n_iter
        self._learning_rate = learning_rate
        self._l2 = l2
        self._neg_samples = neg_samples
        self._use_cuda = use_cuda

        # rank evaluation related
        self.test_sequence = None
        self._candidate = dict()

    @property
    def _initialized(self):
        return self._net is not None

    def _initialize(self, interactions):
        self._num_items = interactions.num_items
        self._num_users = interactions.num_users
        self._num_rates = interactions.num_rates

        self.test_sequence = interactions.test_sequences

        self._net = gpu(Caser(self._num_users,
                              self._num_items,
                              self._num_rates,
                              self.model_args), self._use_cuda)
        print(self._net)

        self._optimizer = optim.Adam(
            self._net.parameters(),
            weight_decay=self._l2,
            lr=self._learning_rate)

    def fit(self, train, test, verbose=False):
        """
        The general training loop to fit the model

        Parameters
        ----------

        train: :class:`spotlight.interactions.Interactions`
            training instances, also contains test sequences
        test: :class:`spotlight.interactions.Interactions`
            only contains targets for test sequences
        verbose: bool, optional
            print the logs
        """

        # convert to sequences, targets and users
        sequences = train.sequences.sequences
        targets = train.sequences.targets
        #print(type(train.sequences.user_ids), len(train.sequences.user_ids), type(train.sequences.rate_ids), len(train.sequences.rate_ids))
        users = train.sequences.user_ids.reshape(-1, 1)
        rates = train.sequences.rate_ids.reshape(-1, 1)

        L, T = train.sequences.L, train.sequences.T

        n_train = sequences.shape[0]

        output_str = 'total training instances: %d' % n_train
        print(output_str)

        if not self._initialized:
            self._initialize(train)

        start_epoch = 0

        for epoch_num in range(start_epoch, self._n_iter):

            t1 = time()

            # set model to training model
            self._net.train()

            users, rates, sequences, targets = shuffle(users, rates,
                                                sequences,
                                                targets)

            negative_samples = self._generate_negative_samples(users, train, n=self._neg_samples * T)

            sequences_tensor = gpu(torch.from_numpy(sequences),
                                   self._use_cuda)
            user_tensor = gpu(torch.from_numpy(users),
                              self._use_cuda)
            rate_tensor = gpu(torch.from_numpy(rates),
                              self._use_cuda)
            item_target_tensor = gpu(torch.from_numpy(targets),
                                     self._use_cuda)
            item_negative_tensor = gpu(torch.from_numpy(negative_samples),
                                       self._use_cuda)

            epoch_loss = 0.0

            for minibatch_num, \
                (batch_sequence,
                 batch_user,
                 batch_rate,
                 batch_target,
                 batch_negative) in enumerate(minibatch(sequences_tensor,
                                                        user_tensor,
                                                        rate_tensor,
                                                        item_target_tensor,
                                                        item_negative_tensor,
                                                        batch_size=self._batch_size)):
                sequence_var = Variable(batch_sequence)
                user_var = Variable(batch_user)
                rate_var = Variable(batch_rate)
                item_target_var = Variable(batch_target)
                item_negative_var = Variable(batch_negative)

                target_prediction = self._net(sequence_var,
                                              user_var,
                                              rate_var,
                                              item_target_var)
                negative_prediction = self._net(sequence_var,
                                                user_var,
                                                rate_var,
                                                item_negative_var,
                                                use_cache=True)

                self._optimizer.zero_grad()
                # compute the binary cross-entropy loss
                positive_loss = -torch.mean(torch.log(F.sigmoid(target_prediction)))
                negative_loss = -torch.mean(torch.log(1 - F.sigmoid(negative_prediction)))
                loss = positive_loss + negative_loss

                epoch_loss += loss.data[0]

                loss.backward()
                self._optimizer.step()

            epoch_loss /= minibatch_num + 1

            t2 = time()
            if verbose and (epoch_num + 1) % 1 == 0:
                precision, recall, mean_aps = evaluate_ranking(self, test, train, k=[1, 5, 10])
                output_str = "Epoch %d [%.1f s]\tloss=%.4f, map=%.4f, " \
                             "prec@1=%.4f, prec@5=%.4f, prec@10=%.4f, " \
                             "recall@1=%.4f, recall@5=%.4f, recall@10=%.4f, [%.1f s]" % (epoch_num + 1,
                                                                                         t2 - t1,
                                                                                         epoch_loss,
                                                                                         mean_aps,
                                                                                         np.mean(precision[0]),
                                                                                         np.mean(precision[1]),
                                                                                         np.mean(precision[2]),
                                                                                         np.mean(recall[0]),
                                                                                         np.mean(recall[1]),
                                                                                         np.mean(recall[2]),
                                                                                         time() - t2)
                print(output_str)
            else:
                output_str = "Epoch %d [%.1f s]\tloss=%.4f [%.1f s]" % (epoch_num + 1,
                                                                        t2 - t1,
                                                                        epoch_loss,
                                                                        time() - t2)
                print(output_str)

    def _generate_negative_samples(self, users, interactions, n):
        """
        Sample negative from a candidate set of each user. The
        candidate set of each user is defined by:
        {All Items} \ {Items Rated by User}

        Parameters
        ----------

        users: array of np.int64
            sequence users
        interactions: :class:`spotlight.interactions.Interactions`
            training instances, used for generate candidates
        n: int
            total number of negatives to sample for each sequence
        """

        users_ = users.squeeze()
        negative_samples = np.zeros((users_.shape[0], n), np.int64)
        if not self._candidate:
            all_items = np.arange(interactions.num_items - 1) + 1  # 0 for padding
            train = interactions.tocsr()
            for user, row in enumerate(train):
                self._candidate[user] = list(set(all_items) - set(row.indices))

        for i, u in enumerate(users_):
            for j in range(n):
                # print(i, u, j)
                x = self._candidate[u]
                negative_samples[i, j] = x[np.random.randint(len(x))]

        return negative_samples

    def predict(self, user_id, item_ids=None):
        """
        Make predictions for evaluation: given a user id, it will
        first retrieve the test sequence associated with that user
        and compute the recommendation scores for items.

        Parameters
        ----------

        user_id: int
           users id for which prediction scores needed.
        item_ids: array, optional
            Array containing the item ids for which prediction scores
            are desired. If not supplied, predictions for all items
            will be computed.
        """

        if self.test_sequence is None:
            raise ValueError('Missing test sequences, cannot make predictions')

        # set model to evaluation model
        self._net.eval()

        sequence = self.test_sequence.sequences[user_id, :]
        sequence = np.atleast_2d(sequence)
        rate_ids = self.test_sequence.rate_ids[user_id, :]
        print("predict", sequence, rate_ids)

        if item_ids is None:
            item_ids = np.arange(self._num_items).reshape(-1, 1)

        sequences = torch.from_numpy(sequence.astype(np.int64).reshape(1, -1))
        item_ids = torch.from_numpy(item_ids.astype(np.int64))
        user_id = torch.from_numpy(np.array([[user_id]]).astype(np.int64))
        rate_id = torch.from_numpy(np.array([[rate_id]]).astype(np.int64))
        
        sequence_var = Variable(gpu(sequences, self._use_cuda))
        item_var = Variable(gpu(item_ids, self._use_cuda))
        user_var = Variable(gpu(user_id, self._use_cuda))
        rate_var =  Variable(gpu(rate_id, self._use_cuda))
        
        out = self._net(sequence_var,
                        user_var,
                        rate_var,
                        item_var,
                        for_pred=True)

        return cpu(out.data).numpy().flatten()


if __name__ == '__main__':
    # write at file
    orig_stdout = sys.stdout
    f = open('log/out.txt', 'w')
    sys.stdout = f
    
    parser = argparse.ArgumentParser()
    # data arguments
    parser.add_argument('--train_root', type=str, default='datasets/ml1m/test/train.txt')
    parser.add_argument('--test_root', type=str, default='datasets/ml1m/test/test.txt')
    parser.add_argument('--L', type=int, default=5)
    parser.add_argument('--T', type=int, default=3)
    # train arguments
    parser.add_argument('--n_iter', type=int, default=1)
    parser.add_argument('--seed', type=int, default=1234)
    parser.add_argument('--batch_size', type=int, default=512)
    parser.add_argument('--learning_rate', type=float, default=1e-3)
    parser.add_argument('--l2', type=float, default=1e-6)
    parser.add_argument('--neg_samples', type=int, default=3)
    parser.add_argument('--use_cuda', type=str2bool, default=True)


    # config = parser.parse_args()
    # model dependent arguments
    parser.add_argument('--d', type=int, default=100)
    parser.add_argument('--nv', type=int, default=4)
    parser.add_argument('--nh', type=int, default=16)
    parser.add_argument('--drop', type=float, default=0.5)
    parser.add_argument('--ac_conv', type=str, default='relu')
    parser.add_argument('--ac_fc', type=str, default='relu')
    config = parser.parse_args()
    # model_config = model_parser.parse_args()
    # model_config.L = config.L

    # set seed
    set_seed(config.seed,
             cuda=config.use_cuda)

    # load dataset
    train = Interactions(config.train_root)
    # transform triplets to sequence representation
    train.to_sequence(config.L, config.T)

    test = Interactions(config.test_root,
                        user_map=train.user_map,
                        item_map=train.item_map,
                        rate_map=train.rate_map)

    print(config)
    #print(model_config)
    # fit model
    model = Recommender(n_iter=config.n_iter,
                        batch_size=config.batch_size,
                        learning_rate=config.learning_rate,
                        l2=config.l2,
                        neg_samples=config.neg_samples,
                        model_args=config,
                        use_cuda=config.use_cuda)

    model.fit(train, test, verbose=True)
    
    print('--------------end--------------\n\n')
    sys.stdout = orig_stdout
    f.close()
